---
layout: none
---
<html lang="en">
<head>
  <title>Eyewear 2021 – The Forth Workshop on Eyewear Computing. Augmenting Social Situations and Democratizing Tools.</title>
  <meta charset="UTF-8">
  <meta name="keywords" content="Human-Computer Interaction, Eyetracking, Open Eyewear, HCI, Workshop, Wearables">
  <meta name="description" content="The Forth Workshop on Eyewear Computing: Augmenting Social Situations and Democratizing Tools.">
  <meta name="author" content="Kirill Ragozin">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <link href="/assets/fv/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="/assets/vendor/aos/dist/aos.css" rel="stylesheet" type="text/css" media="all">
  <link href="/assets/css/overwrite.css" rel="stylesheet" type="text/css" media="all">
</head>
<body class="bg-black-800 saas-template">

<header class="muse-header py-0 py-sm-2">
  <div class="container">
    <nav class="navbar navbar-expand-lg">
      <a class="navbar-brand h6 text-white" href="#">
        Eyewear 2021
      </a>
      <button class="navbar-toggler collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <svg class="menu-icon" data-name="icons/tabler/hamburger" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 16 16">
          <rect data-name="Icons/Tabler/Hamburger background" width="16" height="16" fill="none"/>
          <path d="M15.314,8H.686A.661.661,0,0,1,0,7.368a.653.653,0,0,1,.593-.625l.093-.006H15.314A.661.661,0,0,1,16,7.368a.653.653,0,0,1-.593.626Zm0-6.737H.686A.661.661,0,0,1,0,.632.654.654,0,0,1,.593.005L.686,0H15.314A.661.661,0,0,1,16,.632a.653.653,0,0,1-.593.625Z" transform="translate(0 4)" fill="#ffffff"/>
        </svg>          
        <svg class="menu-close" data-name="icons/tabler/close" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 16 16">
          <rect data-name="Icons/Tabler/Close background" width="16" height="16" fill="none"/>
          <path d="M.82.1l.058.05L6,5.272,11.122.151A.514.514,0,0,1,11.9.82l-.05.058L6.728,6l5.122,5.122a.514.514,0,0,1-.67.777l-.058-.05L6,6.728.878,11.849A.514.514,0,0,1,.1,11.18l.05-.058L5.272,6,.151.878A.514.514,0,0,1,.75.057Z" transform="translate(2 2)" fill="#ffffff"/>
        </svg>          
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav m-auto">
        </ul>
        <a href="#introduction" class="btn btn-lg btn-outline-light mx-2 my-3 my-lg-0">Introduction</a>
        <a href="#program" class="btn btn-lg btn-outline-light mx-2 my-3 my-lg-0">Program</a>
        <a href="#organizers" class="btn btn-lg btn-outline-light mx-2 my-3 my-lg-0">Organizers</a>
        {%- comment -%} <a href="#" class="btn btn-lg btn-danger mx-2 my-3 my-lg-0">Call for participation</a> {%- endcomment -%}
      </div>
    </nav>
  </div>
</header>

<div class="container">
  <section class="muse-section">
    <div class="row align-items-center">
      <div class="col-lg-10 m-auto text-center">
        <div class="my-3">
          <h1><svg xmlns="http://www.w3.org/2000/svg" width="120" height="120" fill="currentColor" class="bi bi-eye px-3" viewBox="0 0 16 16">
  <path d="M16 8s-3-5.5-8-5.5S0 8 0 8s3 5.5 8 5.5S16 8 16 8zM1.173 8a13.133 13.133 0 0 1 1.66-2.043C4.12 4.668 5.88 3.5 8 3.5c2.12 0 3.879 1.168 5.168 2.457A13.133 13.133 0 0 1 14.828 8c-.058.087-.122.183-.195.288-.335.48-.83 1.12-1.465 1.755C11.879 11.332 10.119 12.5 8 12.5c-2.12 0-3.879-1.168-5.168-2.457A13.134 13.134 0 0 1 1.172 8z"/>
  <path d="M8 5.5a2.5 2.5 0 1 0 0 5 2.5 2.5 0 0 0 0-5zM4.5 8a3.5 3.5 0 1 1 7 0 3.5 3.5 0 0 1-7 0z"/>
</svg>
<svg xmlns="http://www.w3.org/2000/svg" width="120" height="120" fill="currentColor" class="bi bi-eye-fill px-3" viewBox="0 0 16 16">
  <path d="M10.5 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 0 1 5 0z"/>
  <path d="M0 8s3-5.5 8-5.5S16 8 16 8s-3 5.5-8 5.5S0 8 0 8zm8 3.5a3.5 3.5 0 1 0 0-7 3.5 3.5 0 0 0 0 7z"/>
</svg>

</h1>
          <h1 class="display-2">Augmenting Social Situations and Democratizing Tools</h1>
          <h2 class="mb-0 pb-md-3 mt-2">Eyewear 2021: The Forth Workshop on Eyewear Computing</h2>
          <h2 class="mb-0 pb-md-3 mt-2">Paper Deadline: 30th June 2021</h2>

          <a href="#call-for-participation" class="btn btn-xl btn-danger-c rounded-pill my-4">Call for participation</a>
        </div>
      </div>
    </div>
  </section>
  
  <section id="introduction" class="muse-section border-gray-800">
    <div class="row g-0">
      <div class="col-lg-10 py-xl-4 my-xl-2 m-auto  bg-gray-200 rounded-12 px-md-5 px-2 py-5 text-black">
        <h2 class="display-4 text-left mb-1 mt-2 mb-md-5 mt-md-4 px-2">Introduction</h2>
        <div class="big px-2 lh-md">
          <p class="py-2">Head-worn sensing, especially embedded in augmented and virtual reality (AR/VR) head-mounted displays and smart glasses is currently increasingly moving away from niche applications and small-scale research prototypes to large-scale consumer adoption (e.g. Oculus Quest 2, Hololens 2, J!NS MEME, Bose Frames).</p>

          <p class="py-2">Significant progress in sensing technologies and modalities have lead to a constant increase of commercially available products and unobtrusive, affordable research prototypes. These recent advances allow us to extend the Eyewear Community to enable large scale in-situ studies, as one of the favored research methodologies in Ubiquitous Computing. One manifestation of this can already be observed in large scale dataset recording, eyewear student competitions and programming seminars.</p>

          <p class="py-2">In this workshop we focus on supporting these large-scale uses of eyewear computing, discussing lessons learned from early deployment and how to empower the community with better hardware/software prototyping tools as well as the establishment of open data sets.</p>


          <p class="py-2">In addition, we will discuss long-term psychological and physical impacts and risks of the technology that become increasingly important with a wider distribution of devices to consumers.</p>

          <p class="py-2">The proposed workshop will bring together researchers from a wide range of disciplines, such as mobile and ubiquitous computing, activity recognition eye tracking, optics, human vision and perception, usability, as well as systems engineering research. This year it will also bring in researchers from neuroscience, psychology and other fields that might want to apply or use the research systems. The workshop is a continuation from 2016/2018/2019 and will focus on discussing on how to democratizing tools for researchers who want to apply eyewear computing (sensing/interaction) in their fields, yet are no wearable computing experts or computer scientists.</p>
        </div>
      </div>
    </div>
  </section>

  <section id="program" class="muse-section border-gray-800">
    <div class="row g-0">
      <div class="col-lg-10 py-xl-4 my-xl-2 m-auto rounded-12 px-md-5 px-2 py-5 text-white kw-bg-blue">
        <h2 class="display-4 text-left mb-1 mt-2 mb-md-0 mt-md-4 px-2">Program</h2>
        <div class="big px-2 lh-md">
          <h2 class="my-3 pt-3 pt-md-5">Workshop activities</h2>
          <h5 class="my-3 mt-4">Welcome <span class="badge badge-warning mx-2 pb-2">10:00 - 11:30</span></h5>
          <p>The workshop will start with an introduction given by the workshop organizers in which they will summarize the workshop's motivation, goals, and outline (10:00-10:15).
This will be followed by short introductory presentations of the workshop participants (in Pecha Kucha style) to get familiar with each other and to get to know the topics they are working on.
The presentation session will be broken into two parts with a short break in between. This will allow enough time to discuss different ideas coming out from the presentations.</p>

<h5 class="my-3 mt-5">Keynotes <span class="badge badge-warning mx-2 pb-2">11:30 - 12:30</span></h5>
<p>One keynote will be given by Prof. Thad Starner (Georgia Tech).</p>
<p>Others will be announced later, stay tuned.</p>


<h5 class="my-3 mt-5">Demonstration Session</h5>
<p>We encourage participants to demonstrate research prototypes related to their submissions. Depending on the number of demonstrations we will hold a separate session or do small show-and-tells in small breaks.</p>

<h5 class="my-3 mt-5">Scenario Development <span class="badge badge-warning mx-2 pb-2">13:00 - 15:30</span></h5>
<p>After the presentations, grouped participants will develop new scenarios. All participants will write notes, which will posted on a virtual whiteboard. In order to sort out the challenges and opportunities for an Open Eyewear Platform, we will create an affinity diagram analysis of the gathered ideas. Group analysis will start at 15:00 and will end at 15:30.</p>

<h5 class="my-3 mt-5">Group Discussion <span class="badge badge-warning mx-2 pb-2">15:30 - 17:30</span></h5>
<p>After the group analysis, we will have a longer coffee break (15:30-16:00) followed by a discussion round on identified challenges and opportunities (16:00-17:00). Organizers will actively engage with the audience to stimulate discussions. We will summarize the key experiences from the workshop and will plan follow up activities (17:00-17:30).</p>

<h2 class="mt-5 pt-5 mb-3 border-top">Post-Workshop Follow Up</h2>
<p>Organizers will document the outcome of the above analysis, and make this information available to the all participants through a shared Google Drive folder. Participants will be invited to an existing Slack channel where they can share papers relevant to the workshop themes. We will start a Data-set directory on based on the participants submissions during the workshop to make it easier to access useful datasets and tools.</p>
        </div>
      </div>
    </div>
  </section>

  <section id="organizers" class="muse-section border-gray-800">
    <div class="row g-0">
      <div class="col-lg-10 py-xl-4 my-xl-2 m-auto  bg-gray-800 rounded-12 px-md-5 px-2 py-5 text-white">
        <h2 class="display-4 text-left mb-1 mt-2 mb-md-5 mt-md-4 px-2">Organizers</h2>
        <div class="big px-2 lh-md">
          <p class="py-2"><strong class="text-info">Kirill Ragozin</strong> is a postdoctoral researcher at Keio University Graduate School of Media Design. His major research contributions are in mixed reality and embodied thermal interactions. His research interests include immersive digital media, eye tracking and interaction design using wearable electronics.
          </p>

          <p class="py-2"><strong class="text-info">Kai Kunze</strong> is a Professor at Keio University Graduate School of Media Design. His major research contributions are in pervasive computing with a focus on augmenting human abilities.
          </p>

          <p class="py-2"><strong class="text-info">Teresa Hirzle</strong> is a fourth-year Ph.D. student at Ulm University. Her research interests lie in analysing the impact of head-worn technology (in particular VR) on user comfort and developing suitable measurement tools thereof.
          </p>

          <p class="py-2"><strong class="text-info">Benjamin Tag</strong> is a postdoctoral researcher and associate lecturer at the School of Computing and Information Systems at the University of Melbourne. His research focuses on the conceptualisation of digital emotion regulation, and investigation of human cognition using biometric sensors and psychological test methods.
          </p>

          <p class="py-2"><strong class="text-info">Yuji Uema</strong> is a researcher at JINS Inc., where he develops smart eyewear and conducts feasibility studies with special focus on HCI, education and medical application. His Ph.D. research at The University of Tokyo includes the analysis and estimation of cognitive load based on eye blinks and eye movement.
          </p>

          <p class="py-2"><strong class="text-info">Enrico Rukzio</strong> is a Professor of Media Informatics at Ulm University. His research focuses on mobile and wearable interaction, computerized eyewear and automotive user interfaces.
          </p>

          <p class="py-2"><strong class="text-info">Jamie A Ward</strong>  is a lecturer at Goldsmiths, University of London. He works on wearable computing, with contributions to topics like social neuroscience, activity recognition, performance evaluation, and applications to real-world problems in health, industry, and the arts.
          </p>
          
        </div>
      </div>
    </div>
  </section>

  <section id="call-for-participation" class="muse-section border-gray-800">
    <div class="row g-0">
      <div class="col-lg-10 py-xl-4 my-xl-2 m-auto kw-bg-cyan rounded-12 px-md-5 px-2 py-5 text-white">
        <h2 class="display-4 text-left mb-1 mt-2 mb-md-5 mt-md-4 px-2 text-warning">Call for participation</h2>
        <div class="big px-2 lh-md">
        <p class="py-2 mb-4">
          Paper Deadline 30th June 2021 over 
           <a href="https://new.precisionconference.com/submissions"> the Precision conference Submissions Website </a>
          Select SIGCHI, UbiComp/ISWC 2021 and the Eyewear Computing Workshop.
          </p>
         <p class="py-2 mb-4">   
          The human face, holding the majority of human senses, provides versatile information about a person's cognitive and affective states. Using head-worn technology, user states, such as reading, walking, detection of fatigue or cognitive load, can be recognized and enable new application scenarios, such as quantified self for the mind.
Besides, significant progress in sensing technologies and modalities have led to a constant increase in unobtrusive and affordable head-worn sensing devices, such as smart glasses like Google Glass or J!NS meme.
With the resulting increasing ubiquity of the technology, new opportunities arise for applications that track social behaviours and interactions between groups of people in real-world settings.</p>

<p class="py-4 pt-2">This workshop aims to identify key factors in large-scale uses of eyewear computing.
More precisely, we are going to summarize lessons learned from early deployment, focus on ways to empower the community with high-quality hardware and software prototyping tools, and will specifically discuss the establishment of open source datasets.
With the wider distribution of head-worn sensing technology to the public, long-term impacts of the technology become increasingly important.
Therefore, we also welcome topics that are concerned with physical or psychological aspects of head-worn sensing devices.
We invite submissions of position papers (2-4 pages in the ACM sigconf format, excluding references) that cover topics such as, but are not limited to:
</p>

<ul class="py-1 pb-4 bg-warning rounded-12 text-black px-5 py-4">
    <li> Open Eyewear Tools and Datasets </li>
    <li> Eyewear sensing and actuation technologies </li>
    <li> Smart Eyewear interactions </li>
    <li> Application scenarios of head-worn sensing/interaction devices </li>
    <li> Impact and Risks of long-term sensing </li>
    <li> Smart Eyewear User Experience Designs </li>
</ul>

<p class="py-4 mt-3">Submissions will go through a single-phase review process with at least 2 reviewers. They will be assessed based on their relevance, originality, and their potential of initiating a fruitful discussion at the workshop. Note, that position papers are not expected to present finished research projects. We rather ask for thought-provoking ideas or initial explorations of a topic. Position papers will be reviewed by two of the workshop organizers. At the workshop, accepted submissions will be presented in a 5-min prerecorded video, following the Pecha Kucha style. At least one author of an accepted submission must attend the workshop. </p>
        </div>
      </div>
    </div>
  </section>
</div>

<!-- Muse Footer, Pt 4, Pt Md 5, Pb 2, Pb Md 4, Mt 4 -->
<footer class="pt-4 pt-md-5 pb-2 pb-md-4 text-center third-footer">
  <div class="container">
    <div class="pt-3 pt-md-4">
      <div class="mb-3">
        <ul class="list-unstyled footer-link">
          <li>
            <a href="#introduction">Introduction</a>
          </li>
          <li>
            <a href="#call-for-participation">Call for participation</a>
          </li>
          <li>
            <a href="#program">Program</a>
          </li>
          <li>
            <a href="#organizers">Organizers</a>
          </li>
        </ul>
      </div>
    </div>
    <hr class="my-4">
    <p class="small text-muted text-center">© 2021, Geist Lab., Keio Media Design.</p>
  </div>
</footer>

<!-- Muse Javascript Plugins -->
<script src="/assets/vendor/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
<script src="/assets/vendor/aos/dist/aos.js"></script>
<script>
  AOS.init ({
    offset: 0,
    once: true,
    easing: 'ease-out'
  });
</script>
</body>
</html>
